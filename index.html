<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Aditya Vora</title>
  
  <meta name="author" content="Aditya Vora">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="kk_oB9YWBIQJXAP8_h68BzBQgJOd0tL-dK5yfSnu5eU" />
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="./static/images/favicon.svg"> 
</head>

<body>

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Aditya Vora</name>
              </p>
              <p>I am a Ph.D student at the <a href="https://gruvi.cs.sfu.ca/">GrUVi Lab</a> of <a href="https://www.sfu.ca/computing.html"> School of Computer Science</a> at <a href="https://www.sfu.ca/">Simon Fraser University</a>, under the supervision of <a href="https://www.cs.sfu.ca/~haoz/">Prof. Hao (Richard) Zhang</a>.
              </p>
              <p>
                Prior to that, I spent couple of years working in industry where I was mainly involved in development of computer vision and machine learning solutions for fire safety and security products. I completed my master's from <a href="https://iitgn.ac.in/">Indian Institute of Technology, Gandhinagar</a>, India where I did my thesis under the supervision of <a href="https://people.iitgn.ac.in/~shanmuga/">Prof. Shanmuganathan Raman</a>. I received my bachelor's degree in Electronics and Communications Engineering from Birla Vishwakarma Mahavidyalaya, India.          
                <!--At Google I've worked on <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
                I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.-->
              </p>
              <p style="text-align:center">
                <a href="mailto:voraaditya898@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/aditya_vora_cv.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=0LO8tDEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/aditya-vora-b66b1a58/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/aditya-vora">GitHub</a> &nbsp/&nbsp
                <a href="https://twitter.com/anvorain">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="static/images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="static/images/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests are in 3D Computer Vision and Graphics, Geometry Processing and Deep Learning.
                <!--{% comment %} I'm interested in Computer Vision/Graphics and Machine Learning in general. Most of my previous research works has mainly focused on visual perception of physical world from images. {% endcomment %}-->
              </p>
            </td>
          </tr>
        </tbody></table>

        <!-- Publication -->
        <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
            <tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Publications</heading>
                </td>
            </tr>
            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
            <tbody>

            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='static/images/divinet.png' width="200" height="100"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <!-- <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865518300473"> -->
                            <papertitle>DiViNeT: 3D Reconstruction from Disparate Views using Neural Template Regularization</papertitle>
                        <!-- </a> -->
                        <br>
                        <strong>Aditya Vora</strong>,
                        <a href="https://agp-ka32.github.io/">Akshay Gadi Patil</a>,
                        <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
                        <br>
                        <em>Neural Information Processing Systems (NeurIPS), </em> 2023
                        <br>
                        <a href="https://arxiv.org/abs/2306.04699">ArXiv</a> /
                        <a href="https://aditya-vora.github.io/divinetpp">Project Page</a> / 
                        <a href="">code</a> /
                        <a href="data/divinet.bib">bibtex</a>
                    </p>
                    <p></p>
                    <p>
                        We present a volume rendering-based neural surface reconstruction method that takes as few as three disparate RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of neural templates to act as surface priors.
                    </p>
                </td>
            </tr>

            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
            <tbody>

            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='static/images/human_pose_transfer.png' width="200" height="100"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <!-- <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865518300473"> -->
                            <papertitle>Deep Appearance Consistent Human Pose Transfer</papertitle>
                        <!-- </a> -->
                        <br>
                        <a href="https://sites.google.com/iitgn.ac.in/ashishtiwari">Ashish Tiwari</a>,
                        <a href="https://zeeshank95.github.io/">Zeeshan Khan</a>,
                        <strong>Aditya Vora</strong>,
                        <a href="https://www.linkedin.com/in/manjuprakash-rama-rao-7a16248/">Manjuprakash Rama Rao</a>,
                        <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a>
                        <br>
                        <em>International Conference of Pattern Recognition (ICPR),</em> 2022
                        <br>
                        <a href="https://ieeexplore.ieee.org/document/9956219">Paper</a> /
                        <a href="data/posetransfer.bib">bibtex</a>
                    </p>
                    <p></p>
                    <p>
                        We present a robut deep architecture for Appearance Consistent person image generation in novel poses. We incorporate a 3 stream network, for image, pose, and appearance. Additionaly we use Gated convolutions and, Non-local attention blocks for generating realistic images.                    </p>
                </td>
            </tr>

            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
            <tbody>

            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='static/images/isc.png' width="225" height="100"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <!-- <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865518300473"> -->
                            <papertitle>Iterative spectral clustering for unsupervised object localization</papertitle>
                        <!-- </a> -->
                        <br>
                        <strong>Aditya Vora</strong>,
                        <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a>
                        <br>
                        <em>Pattern Recognition Letters (PRL),</em> 2018
                        <br>
                        <a href="https://arxiv.org/abs/1706.09719">arxiv</a> /
                        <a href="data/isc.bib">bibtex</a>
                    </p>
                    <p></p>
                    <p>
                      We propose a completely unsupervised algorithm for the same, where we try to exploit the structural differences between the foreground and the background region in an image in order to localize the object in the scene.
                    </p>
                </td>
            </tr>

            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
            <tbody>

            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='static/images/flow-free-vos.png' width="225" height="120"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <!-- <a href="https://link.springer.com/chapter/10.1007/978-981-13-0020-2_4"> -->
                            <papertitle>Flow-Free Video Object Segmentation</papertitle>
                        <!-- </a> -->
                        <br>
                        <strong>Aditya Vora</strong>,
                        <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a>
                        <br>
                        <em>National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG),</em> 2017
                        <br>
                        <a href="https://arxiv.org/abs/1706.09544">arxiv</a> /
                        <a href="data/ffvos.bib">bibtex</a>
                    </p>
                    <p></p>
                    <p>
                      We propose an fully automatic video object segmentation algorithm where localization of object segments are obtained by performing clustering on proposals generated by a segmentation proposal network. Later, the overall temporal consistency is improved using a track and fill method.
                    </p>
                </td>
            </tr>

            </tbody>
        </table>

        <!--preprints-->
        <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
            <tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>(Pr)ePrints</heading>
                </td>
            </tr>
            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
            <tbody>

            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='static/images/fchd.png' width="200" height="80"></div>
                </td>
                <td width="75%" valign="middle">
                    <p>
                        <a href="https://arxiv.org/abs/1809.08766">
                            <papertitle>FCHD: Fast and accurate head detection in crowded scenes</papertitle>
                        </a>
                        <br>
                        <strong>Aditya Vora</strong>,
                        Vinay Chilaka
                        <br>
                        <a href="https://arxiv.org/abs/1809.08766">arxiv</a> /
                        <a href="data/fchd.bib">bibtex</a> /
                        <a href="https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector">code</a>
                    </p>
                    <p></p>
                    <p>
                        A fully convolutional single stage head detector is proposed where the anchor scales are designed by taking effective receptive field into account, hence giving better average precision especially for small heads in crowded scenes.
                    </p>
                </td>
            </tr>

            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
            <tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Patents</heading>
                    <p>Rajkumar Palanivel, Amit Kulkarni, Douglas Beaudet, Manjuprakash Rama Rao, Atul Laxman Katole, <strong> Aditya Vora</strong>, "System and Method for identifying blockages of emergency exits in a building.", US Patent 11388775, 2021</p>
                </td>
            </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
            <tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Teaching</heading>
                    <p>TA - CMPT 985: Neural Fields by <a href="http://theialab.ca/">Prof. Andrea Tagliasacchi</a>
                    <span style="float:right;"> SFU [Summer, 2023]</span>
                    </p>
                    <p>TA - CMPT 361: Introduction to Visual Computing by <a href="https://xbpeng.github.io/">Prof. Jason Peng</a> 
                    <span style="float:right;"> SFU [Spring, 2023]</span>
                    </p>
    
                </td>
            </tr>
            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
            <tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Service</heading>
                    <p><strong> Reviewer </strong></p>
                    <p>CVPR 2024</p>    
                </td>
            </tr>
            </tbody>
        </table>



                <!-- Bottom -->
                <table style="width:100%;vertical-align:center;border:0px;border-spacing:0px;padding:0px">
                  <tr>
                      <td>
                          <br>
                          <!-- <p style="text-align:right;font-size:small;margin-bottom: 0">
                              <a href="https://cseweb.ucsd.edu/~kriegman/">&#10025;</a>
                              <a href="http://pages.ucsd.edu/~ztu/">&#10025;</a>
                              <a href="http://ai.stanford.edu/~ssrinath">&#10025;</a>
                              <a href="https://chenshen.xyz">&#10025;</a>
                              <a href="https://orzyt.cn">&#10025;</a>
                              <a href="http://floatingsong.com/">&#10025;</a>
                          </p> -->
                          <hr style="margin-bottom:0;margin-top: 0">
                          <p style="text-align:left;font-size:10px">
                              <span style="font-size:10px;float:right">
                                  Layout inspired by <a href="https://jonbarron.info/" style="font-size: 10px;">Jon Barron</a>
                                  . Thank you Jon. &copy; 2020
                              </span>
                          </p>
                      </td>
                  </tr>
              </table>

      </td>
    </tr>
  </table>

<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=r3swAGHKxzzKmB63LnLJhERDUEbTbE4FxJfF8bnyAWM"></script> -->

</body>

</html>
